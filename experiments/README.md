# Experiments Directory

This directory contains all outputs generated by training, prediction, and embedding extraction runs.

## Directory Structure

The structure is organized by model type and experiment name:
```
experiments/
├── vae/
│   ├── baseline/
│   │   ├── config.yaml
│   │   ├── checkpoints/
│   │   ├── logs/
│   │   ├── embeddings/
│   │   └── predictions/
│   ├── high_beta/
│   │   ├── config.yaml
│   │   ├── checkpoints/
│   │   └── logs/
│   └── debug/
│       ├── config.yaml
│       ├── checkpoints/
│       └── logs/
├── sae/
│   ├── baseline/
│   │   ├── config.yaml
│   │   ├── checkpoints/
│   │   └── logs/
│   └── sparse_heavy/
│       ├── config.yaml
│       ├── checkpoints/
│       └── logs/
└── custom_model/
    └── experiment_001/
        ├── config.yaml
        ├── checkpoints/
        └── logs/
```

## How Directories Are Created

The directory structure is determined by your config:
```yaml
experiment:
  name: "baseline"              # Becomes the experiment folder name
  output_dir: "./experiments/vae"   # Base directory
```

**Result:** `./experiments/vae/baseline/`

### Examples
```yaml
# Experiment 1
experiment:
  name: "baseline"
  output_dir: "./experiments/vae"
# Creates: ./experiments/vae/baseline/

# Experiment 2
experiment:
  name: "high_beta"
  output_dir: "./experiments/vae"
# Creates: ./experiments/vae/high_beta/

# Experiment 3
experiment:
  name: "run_001"
  output_dir: "./experiments/sae"
# Creates: ./experiments/sae/run_001/
```

## Contents of Each Experiment Directory

### Training Run Output

After running `python scripts/train.py configs/train_vae.yaml`:
```
experiments/vae/baseline/
├── config.yaml                 # Copy of the config used for this run
├── checkpoints/
│   ├── best_model.pt          # Best model based on validation metric
│   ├── last_model.pt          # Most recent epoch
│   └── epoch_*.pt             # Individual epochs (if save_strategy="all")
└── logs/
    └── training.log           # Training logs
```

### Checkpoint Contents

Each checkpoint file (`.pt`) contains:
```python
{
    'epoch': int,                      # Epoch number
    'model_state_dict': dict,          # Model weights
    'optimizer_state_dict': dict,      # Optimizer state
    'metric_value': float,             # Value of tracked metric
    'config': dict,                    # Full config for reproducibility
    'random_state': dict               # Random seed states
}
```

### Embedding Extraction Output

After running `python scripts/extract_embeddings.py configs/extract_embeddings.yaml`:
```
experiments/vae/baseline/
├── ... (training outputs)
└── embeddings/
    ├── train_embeddings.npy       # Training set embeddings
    ├── val_embeddings.npy         # Validation set embeddings
    └── test_embeddings.npy        # Test set embeddings
```

**Embedding file format:** NumPy array with shape `(n_samples, embedding_dim)`

### Prediction Output

After running `python scripts/predict.py configs/predict.yaml`:
```
experiments/vae/baseline/
├── ... (training outputs)
└── predictions/
    └── test_predictions.npy       # Test set predictions
```

**Predictions file format:** NumPy array containing model outputs

## Checkpoint Strategies

Configured via `checkpoint.save_strategy` in your config:

### `save_strategy: "best"`
Saves only the best model based on validation metric:
```
checkpoints/
└── best_model.pt
```

### `save_strategy: "last"`
Saves only the most recent epoch:
```
checkpoints/
└── last_model.pt
```

### `save_strategy: "all"`
Saves every epoch plus the best:
```
checkpoints/
├── best_model.pt
├── last_model.pt
├── epoch_001.pt
├── epoch_002.pt
├── epoch_003.pt
└── ...
```

## MLflow Integration

If MLflow is enabled in your config, artifacts are also logged to MLflow:
```yaml
mlflow:
  tracking_uri: "https://mlflow.example.com"
  experiment_name: "vae_experiments"
```

MLflow tracks:
- Training and validation metrics
- Config file
- Best model checkpoint
- Custom metrics and artifacts

## Typical Workflow

### 1. Training Phase
```bash
python scripts/train.py configs/train_vae.yaml
```

**Creates:**
```
experiments/vae/baseline/
├── config.yaml
├── checkpoints/best_model.pt
└── logs/training.log
```

### 2. Embedding Extraction Phase
```bash
python scripts/extract_embeddings.py configs/extract_embeddings.yaml
```

**Adds:**
```
experiments/vae/baseline/
└── embeddings/
    ├── train_embeddings.npy
    ├── val_embeddings.npy
    └── test_embeddings.npy
```

### 3. Downstream Training (SAE on embeddings)
```bash
python scripts/train.py configs/train_sae.yaml
```

**Creates new experiment:**
```
experiments/sae/baseline/
├── config.yaml
├── checkpoints/best_model.pt
└── logs/training.log
```

### 4. Prediction Phase
```bash
python scripts/predict.py configs/predict.yaml
```

**Adds:**
```
experiments/vae/baseline/
└── predictions/
    └── test_predictions.npy
```

## Git Integration

The `experiments/` directory is typically added to `.gitignore` since it contains:
- Large model checkpoints
- Generated embeddings
- Prediction outputs

**What to commit:**
- Config files (in `configs/`)
- Code (in `src/` and `scripts/`)

**What NOT to commit:**
- Experiment outputs (in `experiments/`)
- Data files

**Exception:** You might want to commit the `experiments/README.md` file itself to document the structure.

## Cleaning Up

To remove old experiments:
```bash
# Remove specific experiment
rm -rf experiments/vae/old_experiment/

# Remove all experiments for a model type
rm -rf experiments/vae/

# Keep only checkpoints, remove logs
find experiments/ -name "logs" -type d -exec rm -rf {} +
```

## Best Practices

### Naming Conventions

Use descriptive experiment names that indicate what's being tested:
```yaml
# Good names
name: "baseline"
name: "high_beta_1.5"
name: "latent_dim_64"
name: "no_regularization"
name: "2024_01_15_experiment_01"

# Less descriptive
name: "test"
name: "run1"
name: "temp"
```

### Organization

Group related experiments under the same `output_dir`:
```yaml
# All VAE experiments together
output_dir: "./experiments/vae"

# All SAE experiments together
output_dir: "./experiments/sae"

# Dated experiment batches
output_dir: "./experiments/2024_01_15"
```

### Reproducibility

Each experiment directory contains `config.yaml` - this is your source of truth for reproducing results:
```bash
# Reproduce an experiment
python scripts/train.py experiments/vae/baseline/config.yaml
```

### Disk Space Management

- Use `save_strategy: "best"` for production runs (saves space)
- Use `save_strategy: "all"` only for important experiments
- Regularly clean up debug/test experiments
- Consider compressing old experiment directories:
```bash
  tar -czf experiments_archive_2024_01.tar.gz experiments/vae/
```

## Troubleshooting

### "Directory already exists" Error

If you rerun an experiment with the same name and output_dir, the framework will:
- **Option 1:** Overwrite (default behavior)
- **Option 2:** Append timestamp (if configured)

To avoid conflicts, use unique experiment names.

### Missing Checkpoints

If checkpoints are missing:
1. Check if training completed successfully (see `logs/training.log`)
2. Verify `checkpoint.save_strategy` in config
3. Check disk space (training might have failed due to full disk)

### Large Directory Size

If experiment directories are too large:
1. Check `save_strategy: "all"` - this saves every epoch
2. Remove unnecessary epoch checkpoints:
```bash
   rm experiments/vae/baseline/checkpoints/epoch_*.pt
   # Keep only best_model.pt and last_model.pt
```
3. Compress embeddings:
```bash
   gzip experiments/vae/baseline/embeddings/*.npy
```

## Summary

- Each experiment gets its own directory: `{output_dir}/{name}/`
- Training outputs: checkpoints and logs
- Embedding extraction: separate `embeddings/` subdirectory
- Predictions: separate `predictions/` subdirectory
- Config is always saved for reproducibility
- Use descriptive names and organize by model type